\chapter{Appendix}
\label{ch:appAlabel}
\section{Directional Filtering Code}
\begin{lstlisting}[language=Matlab, label=lst:delayDifference]
% Gives distance for the signal to travel to each microphones 
% 1st microphone is left 
% 2nd microphone is right
function [dist1, dist2] = dDifference(thetaRad,gap)
d1=sin(thetaRad)+gap/2; %from y to 1'st mic           |theta
d2=sin(thetaRad)-gap/2; %from y to 2'nd mic           |     o
h=cos(thetaRad);       %                              |    /|
dist1=sqrt(d1^2+h^2);  %                              |   / |
dist2=sqrt(d2^2+h^2);  %                              |  /  |
end                    %                              | /   |
                       %                           1  |/ 2  |
                       %                           o  x  o  y
                       %                           |<--->|
                       %                             gap
\end{lstlisting}

\begin{lstlisting}[language=Matlab, label=lst:ratioNsamples]
%% Function gives ratio and delay, which can be used to realisticaly delay and add realistic gain to the signals
%  Input: angle of the source(degrees), 
%  angle is possitive in clockwise direction and 
%  0 is right in the center, 90 is all the way to the right
%  -90 would be all the way to the left
%  and a gap between microphones in meters
%
%  Output:
%  Ratio (r) will give a gain by how much we should multiply source,
%  captured by Right microphone in relation to Left microphone
%  
%  Delay will give delay at which source, going to the Right microphone
%  should be shifted in relation to Left microphone
%  inspiration: 
%  http://www.sengpielaudio.com/calculator-gainloss.htm
%  http://www.sengpielaudio.com/calculator-distance.htm

function [r, d] = ratioNsamples(theta,gap)
v=343.22;
thetaRad = deg2rad(theta);
%% Distances to left and right mics
[distL,distR] = dDifference(thetaRad, gap); % First source distances
%% Ratio to multiply Right microphone
r=distL/distR;
%% Delay in samples
d = round(((distL/v)-(distR/v))*48000);
end
\end{lstlisting}
\begin{lstlisting}[language=Matlab, label=1st:realSamplesSeparationFunction]
function [separated,shifted] = separation_of_real_samples(delay, Right, Left,l)
%% delay so the phase matches

for i=1:l-delay
    shifted(i)=Left(i+delay);
end
shifted=transpose(shifted);
%% substract signals from each other = get noise
for i=1:l-delay
    separated(i)=shifted(i)-Right(i);
end
 separated=transpose(separated);
\end{lstlisting}

\begin{lstlisting}[language=Matlab, label=lst:realSamplesSeparation]
clc;
clear;
load('already_shifted_vectors.mat')
l=length(dataLL)
delay=8
[Separated,shiftedL]=separation_of_real_samples(delay, dataLL, dataRL,l);
\end{lstlisting}

\begin{lstlisting}[language=Matlab, label=lst:shiftFunction]
%% SHIFT FUNCTION 
% this function shifts the signal by the amount samples and returns a shifted signal
% input the signal the the delay in samples, negative or positive

function[shifted] = shiftFunction(samples, signal)
shifted = zeros(size(signal));
if samples > 0
   shifted(samples+1:end) = signal(1:end - samples);
 elseif samples < 0
   shifted(1:end + samples) = signal(-samples + 1:end);
 end
end
\end{lstlisting}

\begin{lstlisting}[language=Matlab, label=lst:syntheticSamplesCode]
%% CLEANUP

close all;
clc;
clear;
gap=0.152;

%% IMPORT SIGNALS

signal1 = ['C:\P5Project\Adobe Audition\1micRecording\1micRecording_Recorded\Razvan.wav'];
[a,Freq1] = audioread(signal1);
w = (1:length(a)) / Freq1;

signal2 = ['C:\P5Project\Adobe Audition\1micRecording\1micRecording_Recorded\Paulius.wav'];
[b,Freq2] = audioread(signal2);
x = (1:length(b)) / Freq2;


%% RESIZE

[r1, d1] = ratioNsamples(40, gap);
[r2, d2] = ratioNsamples(-30, gap);

resizedA = a(1:580000);
resizedB = b(1:580000);

%% SHIFT AND GAIN
rightMicLeftChatRazvan = resizedA;
leftMicLeftChatRazvan = shiftFunction(d1, resizedA);
rightMicLeftChatRazvan = r1 * rightMicLeftChatRazvan;

rightMicRightChatPaulius = resizedB;
leftMicRightChatPaulius = shiftFunction(d2, resizedB);
rightMicRightChatPaulius = r2 * rightMicRightChatPaulius;

%% ADD SIGNALS FOR LEFT AND RIGHT MIC

rightMicBothChatRazvanPaulius = rightMicLeftChatRazvan + rightMicRightChatPaulius;
leftMicBothChatRazvanPaulius = leftMicLeftChatRazvan + leftMicRightChatPaulius;

%% FILTER

shiftedLeftMicBothChatRazvanPaulius = shiftFunction(-d1, leftMicBothChatRazvanPaulius);
shiftedLeftMicBothChatRazvanPaulius1 = shiftFunction(-d2, leftMicBothChatRazvanPaulius);

subtractedSignal1 = shiftedLeftMicBothChatRazvanPaulius - rightMicBothChatRazvanPaulius;
subtractedSignal2 = shiftedLeftMicBothChatRazvanPaulius1 - rightMicBothChatRazvanPaulius;

%% SAVE SIGNAL

filename1 = 'subtractedSignalR.flac';
audiowrite(filename1,subtractedSignal1,48000);
filename2 = 'subtractedSignalP.flac';
audiowrite(filename2,subtractedSignal2,48000);
filename3 = 'leftMicBothChat.flac';
audiowrite(filename3,leftMicBothChatRazvanPaulius,48000);
filename4 = 'rightMicBothChat.flac';
audiowrite(filename4, rightMicBothChatRazvanPaulius,48000);

%% CLEAR VARIABLES

clearvars Freq1 Freq2 Freq3 Freq4 signal1 signal2 signal3 signal4 w x y z
clearvars filename1 filename2 filename3 filename4 a b resizedA resizedB
\end{lstlisting}





\section{Neural Network Code}
\begin{lstlisting}[language=Python, label=lst:imports]
#basic imports

import matplotlib.pyplot as plt
import os
import random
import numpy as np
import math, random
import tensorflow as tf
#os.environ['KERAS_BACKEND'] = 'tensorflow'
import keras as ks
import soundfile as sf
import sounddevice as sd
import python_speech_features

%matplotlib inline
np.random.seed(0)
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:speechsample]
#Finding speech sample
class speech:
    
    def choose_random(self):
        path = r"C:\Users\Darius\Desktop\sound samples\training data for RNN\LibriSpeech\train-clean-100"
        for i in range (0,3):
            files = os.listdir(path)
            index = random.randrange(0, len(files))
            while  files[index].endswith('.txt'):
                #index = index + 1
                index = random.randrange(0, len(files))
            path  = os.path.join(path, files[index])
        file_path = path
        self.sample, fs2 = sf.read(file_path)
        
        self.size = len(self.sample)
        self.nfcc = python_speech_features.mfcc(self.sample,18000,0.025,0.01,24)

    def show_sample(self):
        print(file_path)
        plt.plot(self.sample)
    def listen_sample(self):
        sd.play(self.sample) 
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:noisesample]
#Finding noise sample
class noise:
    def __init__(self,size):
        self.size = size
    
    def choose_random(self):
        path = r"C:\Users\Darius\Desktop\sound samples\training data for RNN\rnnoise_contributions"
        files = os.listdir(path)
        index = random.randrange(0, len(files))
        if  files[index].endswith('.txt'):
            index = index -1
        file_path = os.path.join(path, files[index])
        self.sample, fs = sf.read(file_path, channels=2, samplerate=24100,format='RAW', subtype='PCM_16')

        i=0
        while i!=1:
            random_sequence = random.randrange(0,len(self.sample))
            if random_sequence + self.size < len(self.sample):
                i=1
        self.sample = self.sample[random_sequence:]
        self.sample = self.sample[:self.size ,:1]
        
        self.nfcc = python_speech_features.mfcc(self.sample,18000,0.025,0.01,24)
        
    def show_sample():
        print(file_path)
        plt.plot(self.sample)
    def listen_sample(self):
        sd.play(self.sample) 
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:imputsample]
#making input sample 
class combined_samples:
    def __init__(self,speech_sample,noise_sample):
        self.speech_sample = speech_sample
        self.noise_sample  = noise_sample
    
    def combine_samples(self):
        self.sample = [len(self.speech_sample)] * 0
        self.sample = np.empty([len(self.speech_sample)])

        for i in range(len(self.speech_sample)):
            self.sample[i] = self.speech_sample[i] + self.noise_sample[i]
        
        self.size = len(self.sample)
        self.nfcc = python_speech_features.mfcc(self.sample,18000,0.025,0.01,24)
        
    def show_sample(self):
        plt.plot(self.sample)
    def listen_sample(self):
        sd.play(self.sample) 
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:trainingdata]
# Training Data
class training_data:
    def get_data(self):
        self.speechD = speech()
        self.speechD.choose_random()
        
        self.noiseD = noise(self.speechD.size)
        self.noiseD.choose_random()
        
        self.combined = combined_samples(self.speechD.sample , self.noiseD.sample)
        self.combined.combine_samples()
        
        self.input = self.combined.nfcc
        self.output= self.speechD.nfcc
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:gru nn]
#Creating The GRU NN
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation
from keras.layers import Activation, GRU, Flatten, Embedding, Bidirectional

input_dim = 24
max_dim = 2000


model = Sequential()

model.add(Bidirectional(GRU(input_dim,input_shape=(None,input_dim),activation='tanh',return_sequences=True)))

model.add(Dropout(0.15))

model.add(Dense(units=input_dim))

model.add(Activation("linear"))


model.build()
model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])

\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:summary]
#printing the model and setting up the call backs
model.summary()

logs_dir= './event_logs_new'
tbCallBack=ks.callbacks.TensorBoard(log_dir='./event_logs_new', histogram_freq=0,
                            write_graph=True, write_images=True)

from keras.utils.vis_utils import plot_model
os.environ["PATH"] += os.pathsep + r"\\Users\\Darius\\Desktop\\RNN stuff\\graphviz\\"
import pydot_ng as pydot
plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)

\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:datasetcreation]
#dataset creation
nrSamples=3000
asta1=np.zeros(shape=(nrSamples,2000,24))
asta2=np.zeros(shape=(nrSamples,2000,24))
for i in range(nrSamples):
    for j in range(len(d.input)):
        for h in range(len(d.input[j])):
            asta1[i][j][h]=d.input[j][h]
            asta2[i][j][h]=d.output[j][h]
    if(i%100==0):
        print(i)
    d.get_data()
            
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:imports]
#basic imports

import matplotlib.pyplot as plt
import os
import random
import numpy as np
import math, random
import tensorflow as tf
#os.environ['KERAS_BACKEND'] = 'tensorflow'
import keras as ks
import soundfile as sf
import sounddevice as sd
import python_speech_features

%matplotlib inline
np.random.seed(0)
\end{lstlisting}

\begin{lstlisting}[language=Python, label=lst:training]
#basic training the neural network

model.fit(asta,asta2,callbacks=[TrainValTensorBoard(write_graph=True)],epochs=60,batch_size=20,validation_data=(valid1,valid2))
\end{lstlisting}

